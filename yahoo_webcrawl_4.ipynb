{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import MeCab\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment set up\n",
    "os.environ[\"LANG\"] = \"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751067cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN TEXT COLUMN (4)\n",
    "\n",
    "url_df = pd.read_csv('url_df.csv')\n",
    "url_df = url_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "def remove_url(text):\n",
    "    url_regex = re.compile(r'[\\u3040-\\u30ff\\u3400-\\u4dbf\\u4e00-\\u9fff\\uf900-\\ufaff\\uff66-\\uff9f]')\n",
    "    http = re.search('htt', text)\n",
    "    www = re.search('www.', text)\n",
    "    if (http is None) & (www is None):\n",
    "        return text\n",
    "    elif http is None:\n",
    "        object_span_www = www.span()[0]\n",
    "        for w in range(object_span_www, len(text)):\n",
    "            url_search = url_regex.search(text[w])\n",
    "            if (url_search is not None) or (w == (len(text)-1)):\n",
    "                end_index = w\n",
    "                url_string = text[object_span_www:end_index]\n",
    "                cleaned_text = text.replace(url_string, '')\n",
    "                return cleaned_text\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    else:\n",
    "        object_span_http = http.span()[0]\n",
    "        for z in range(object_span_http, len(text)):\n",
    "            url_search = url_regex.search(text[z])\n",
    "            if (url_search is not None) or (z == (len(text)-1)):\n",
    "                end_index = z\n",
    "                url_string = text[object_span_http:end_index]\n",
    "                cleaned_text = text.replace(url_string, '')\n",
    "                return cleaned_text\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "segmented_text = []\n",
    "for i in range(len(url_df)):\n",
    "    print('Processing ' + str(i) + ' out of ' + str(len(url_df)))\n",
    "    old_text = url_df['text'][i]\n",
    "    new_text = old_text.replace(\"\\n\", \"\") # remove new line\n",
    "    new_text = new_text.replace(\"\\'\", '')\n",
    "    new_text = re.sub(r'ー{2,}', '', new_text) # remove repetitions of ー\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = remove_url(new_text)\n",
    "    new_text = new_text.replace('(笑)', '')\n",
    "    new_text = new_text.replace('（笑）', '')\n",
    "    new_text = re.sub(r'[0-9]+', '', new_text)\n",
    "    new_text = re.sub(r'[０-９]+', '', new_text)\n",
    "\n",
    "    new_text = new_text.replace(\"・・・\", '。')\n",
    "    new_text = new_text.replace(\"…\", '。')\n",
    "    new_text = new_text.replace(\"!?\", '。')\n",
    "    new_text = new_text.replace(\"?!\", '。')\n",
    "    new_text = new_text.replace(\"！？\", '。')\n",
    "    new_text = new_text.replace(\"？！\", '。')\n",
    "    new_text = new_text.replace(\"?)\", '。')\n",
    "    new_text = new_text.replace(\"？）\", '。')\n",
    "    new_text = new_text.replace(\"!)\", '。')\n",
    "    new_text = new_text.replace(\"！）\", '。')\n",
    "    new_text = new_text.replace(\"？\", '。')\n",
    "    new_text = new_text.replace(\"！\", '。')\n",
    "    new_text = new_text.replace(\"?\", '。')\n",
    "    new_text = new_text.replace(\"!\", '。')\n",
    "    new_text = new_text.replace(\"<break>\", '。')\n",
    "\n",
    "    new_text = re.sub(r'[^ぁ-んァ-ン一-龯。ーA-Za-z]', '', new_text) # remove all symbols other than english and japanese characters\n",
    "\n",
    "    new_text = new_text.split('。')\n",
    "    if '' in new_text:\n",
    "        new_text[:] = (value for value in new_text if value != '')\n",
    "    segmented_text = segmented_text + [new_text]\n",
    "\n",
    "url_df['segmented_text'] = segmented_text\n",
    "\n",
    "sequence_num = []\n",
    "for i in range(len(url_df)):\n",
    "    print('Processing ' + str(i) + ' out of ' + str(len(url_df)))\n",
    "    sequence_num = sequence_num + [len(url_df['segmented_text'][i])]\n",
    "\n",
    "url_df['sequence_length'] = sequence_num\n",
    "url_df = url_df.drop_duplicates(subset=['url'], keep=\"first\")\n",
    "url_df = url_df.drop_duplicates(subset=['text'], keep=\"first\")\n",
    "\n",
    "url_df = url_df.reset_index(drop=True)\n",
    "\n",
    "segmented_text_joined = []\n",
    "for m in range(len(url_df)):\n",
    "    print('Processing ' + str(m) + ' out of ' + str(len(url_df)))\n",
    "    joined_text = ''.join(url_df['segmented_text'][m])\n",
    "    segmented_text_joined = segmented_text_joined + [joined_text]\n",
    "\n",
    "url_df['segmented_text_joined'] = segmented_text_joined\n",
    "url_df.to_csv('url_df_cleaned.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEGMENT THE TEXT INTO SENTENCE SEQUENCES (5)\n",
    "\n",
    "seed_words = []\n",
    "urls = []\n",
    "seq_sentence_ns = []\n",
    "texts = []\n",
    "ranks = []\n",
    "for i in range(len(url_df)):\n",
    "    print('Processing ' + str(i) + ' out of ' + str(len(url_df)))\n",
    "    seed_word = [url_df['seed_word'][i]] * url_df['sequence_length'][i]\n",
    "    seed_words = seed_words + seed_word\n",
    "    rank = [url_df['rank'][i]] * url_df['sequence_length'][i]\n",
    "    ranks = ranks + rank\n",
    "    url = [url_df['url'][i]] * url_df['sequence_length'][i]\n",
    "    urls = urls + url\n",
    "    segmented_text = url_df['segmented_text'][i]\n",
    "    segmented_text = ast.literal_eval(segmented_text)\n",
    "    for j in range(url_df['sequence_length'][i]):\n",
    "        seq_sentence_n = j + 1\n",
    "        seq_sentence_ns = seq_sentence_ns + [seq_sentence_n]\n",
    "        text = segmented_text[j]\n",
    "        texts = texts + [text]\n",
    "\n",
    "\n",
    "seq_df = pd.DataFrame(columns=['seed_word', 'rank', 'url', 'seq_sentence_n', 'cleaned_segmented_text'])\n",
    "seq_df = seq_df.assign(seed_word=seed_words, rank=ranks, url=urls, seq_sentence_n=seq_sentence_ns, cleaned_segmented_text=texts)\n",
    "\n",
    "seq_df.to_csv('sequence_webcrawl_cleaned_df.csv', sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d74fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE NOUNS LIST FOR TF-IDF (6)\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "nouns_list = []\n",
    "for i in range(len(url_df)):\n",
    "    print('Processing ' + str(i) + ' out of ' + str(len(url_df)))\n",
    "    mecab_parsed = mecab.parse(url_df['segmented_text_joined'][i])\n",
    "    parses_1 = mecab_parsed.split('\\n')\n",
    "\n",
    "    for parse in parses_1:\n",
    "        parse_1 = parse.split('\\t')\n",
    "\n",
    "        for par in parse_1:\n",
    "            par_1 = par.split(',')[0]\n",
    "\n",
    "            if par_1 == '名詞':\n",
    "                noun = parse.split()[0]\n",
    "                nouns_list = nouns_list + [noun]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "cleaned_nouns_list = []\n",
    "for j in range(len(nouns_list)):\n",
    "    print('Processing ' + str(j) + ' out of ' + str(len(nouns_list)))\n",
    "    if (nouns_list[j].isalpha()) and (len(nouns_list[j]) <= 2):\n",
    "        continue\n",
    "    else:\n",
    "        cleaned_nouns_list = cleaned_nouns_list + [nouns_list[j]]\n",
    "\n",
    "cleaned_nouns_list = list(set(cleaned_nouns_list))\n",
    "\n",
    "with open(\"nouns_list_all.txt\", \"w\") as output:\n",
    "    output.write(str(cleaned_nouns_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adde328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TERM FREQUENCY FOR EACH NOUN OVER THE SAME SEED WORDS (8)\n",
    "\n",
    "word_df = pd.read_csv('word_df.csv')\n",
    "word_df = word_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "txt_file = open(\"nouns_list_all.txt\", \"r\")\n",
    "file_content = txt_file.read()\n",
    "nouns_list = ast.literal_eval(file_content)\n",
    "txt_file.close()\n",
    "\n",
    "concat_texts = []\n",
    "for i in range(len(word_df)):\n",
    "    concat_seed_text = ''\n",
    "    for j in range(len(url_df)):\n",
    "        if word_df['word'][i] == url_df['seed_word'][j]:\n",
    "            concat_seed_text = concat_seed_text + url_df['segmented_text_joined'][j]\n",
    "        else:\n",
    "            continue\n",
    "    concat_texts = concat_texts + [concat_seed_text]\n",
    "\n",
    "if len(concat_texts) == len(word_df):\n",
    "    print('LOOKS GOOD')\n",
    "else:\n",
    "    print('STOP! ERROR')\n",
    "\n",
    "\n",
    "tf_count = []\n",
    "for n in range(len(concat_texts)):\n",
    "    tf_count_by_seed = []\n",
    "    for m in range(len(nouns_list)):\n",
    "        num_count = concat_texts[n].count(nouns_list[m])\n",
    "        tf_count_by_seed = tf_count_by_seed + [num_count]\n",
    "    tf_count = tf_count + [tf_count_by_seed]\n",
    "\n",
    "\n",
    "if len(concat_texts) == len(tf_count):\n",
    "    print('LOOKS VERY VERY GOOD')\n",
    "else:\n",
    "    print('STOP! ERROR')\n",
    "\n",
    "\n",
    "header_vector = []\n",
    "for p in range(len(nouns_list)):\n",
    "    header_name = 'tf_by_seed(' + nouns_list[p] + ')'\n",
    "    header_vector = header_vector + [header_name]\n",
    "\n",
    "tf_df = pd.DataFrame(tf_count, columns=header_vector)\n",
    "tf_df.insert(loc=0, column='seed_word', value=word_df['word'])\n",
    "\n",
    "tf_df.to_csv('tf_by_seed_df.csv', sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DOCUMENT FREQUENCY FOR EACH NOUN (DF): frequency of words over all documents (9)\n",
    "\n",
    "full_string = ''\n",
    "for i in range(len(url_df)):\n",
    "    print('Processing ' + str(i) + ' out of ' + str(len(url_df)))\n",
    "    full_string = full_string + url_df['segmented_text_joined'][i]\n",
    "\n",
    "df_count = []\n",
    "for j in range(len(nouns_list)):\n",
    "    print('Processing ' + str(j) + ' out of ' + str(len(nouns_list)))\n",
    "    num_count = full_string.count(nouns_list[j])\n",
    "    df_count = df_count + [num_count]\n",
    "\n",
    "\n",
    "data = {'words': nouns_list,\n",
    "        'document_frequency': df_count\n",
    "        }\n",
    "\n",
    "df_df = pd.DataFrame(data)\n",
    "\n",
    "df_df.to_csv('df_by_all.csv', sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE IDF (10)\n",
    "\n",
    "df_df = pd.read_csv('df_by_all.csv')\n",
    "df_df = df_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# get number of documents (25281)\n",
    "N_value = len(url_df)\n",
    "\n",
    "# get idf\n",
    "idf_list = []\n",
    "for i in range(len(df_df)):\n",
    "    print('Processing ' + str(i) + ' out of ' + str(len(df_df)))\n",
    "    n_value = df_df['document_frequency'][i]\n",
    "    x = (N_value - n_value + 0.5)/(n_value + 0.5)\n",
    "    idf = math.log(x, 10)\n",
    "    idf_list = idf_list + [idf]\n",
    "\n",
    "df_df['idf'] = idf_list\n",
    "df_df.to_csv('df_by_all.csv', sep=',', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba01229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TF-IDF DATAFRAME (11)\n",
    "\n",
    "tf_by_seed_df = pd.read_csv('tf_by_seed_df.csv')\n",
    "tf_by_seed_df = tf_by_seed_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df_df = pd.read_csv('df_by_all.csv')\n",
    "df_df = df_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "word_df = pd.read_csv('word_df.csv')\n",
    "word_df = word_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "txt_file = open(\"nouns_list_all.txt\", \"r\")\n",
    "file_content = txt_file.read()\n",
    "nouns_list = ast.literal_eval(file_content)\n",
    "txt_file.close()\n",
    "\n",
    "tf_idf_list = []\n",
    "for i in range(len(word_df['word'])):\n",
    "    print('Processing ' + str(i) + ' out of ' + str(len(word_df)))\n",
    "    tf_idf_by_seed = []\n",
    "    for j in range(len(nouns_list)):\n",
    "        noun = nouns_list[j]\n",
    "        tf_header = 'tf_by_seed(' + noun + ')'\n",
    "        tf = tf_by_seed_df[tf_header][i]\n",
    "        idf_index = df_df[df_df['words'] == noun].index.values\n",
    "        idf_series = df_df['idf'][idf_index].values\n",
    "        idf = idf_series[0]\n",
    "        tf_idf = tf * idf\n",
    "        tf_idf_by_seed = tf_idf_by_seed + [tf_idf]\n",
    "    tf_idf_list = tf_idf_list + [tf_idf_by_seed]\n",
    "\n",
    "tf_idf_df = pd.DataFrame(tf_idf_list, columns=nouns_list)\n",
    "tf_idf_df.insert(loc=0, column='seed_word', value=word_df['word'])\n",
    "\n",
    "tf_idf_df.to_csv('tf_idf_by_seed_df.csv', sep=',', encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
